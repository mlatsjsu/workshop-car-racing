{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04167364,  0.04348699, -0.03316118,  0.04101069])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "epochs = 20\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 32\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_size),\n",
    "    nn.Sigmoid()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.9285326e-01,  1.4838560e+38,  1.3352109e-01,  3.9376048e+36],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGrad(policy, states, rew, actions):\n",
    "    policy.zero_grad()\n",
    "    expRewrads = [] # [sum(rew)] * len(rew)\n",
    "    # sum of the following rewards\n",
    "    for i in range(len(rew)):\n",
    "        expRewrads.append(sum(rew[i:]))\n",
    "    _st = torch.as_tensor(states,  dtype=torch.float32, device=device)\n",
    "    _ac = torch.as_tensor(actions, dtype=torch.int32, device=device)\n",
    "    out = Categorical(policy(_st)).log_prob(_ac)\n",
    "    r = torch.as_tensor(expRewrads, dtype=torch.float32, device=device)\n",
    "    grad = -(out * r).mean()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainig loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg rewards: 22.13274336283186\n",
      "avg rewards: 24.666666666666668\n",
      "avg rewards: 26.230366492146597\n",
      "avg rewards: 30.180722891566266\n",
      "avg rewards: 40.144\n",
      "avg rewards: 50.656565656565654\n",
      "avg rewards: 102.36734693877551\n",
      "avg rewards: 142.5\n",
      "avg rewards: 122.4047619047619\n",
      "avg rewards: 133.1578947368421\n",
      "avg rewards: 136.32432432432432\n",
      "avg rewards: 156.625\n",
      "avg rewards: 162.80645161290323\n",
      "avg rewards: 169.36666666666667\n",
      "avg rewards: 150.26470588235293\n",
      "avg rewards: 181.03571428571428\n",
      "avg rewards: 186.40740740740742\n",
      "avg rewards: 189.66666666666666\n",
      "avg rewards: 194.76923076923077\n",
      "avg rewards: 199.30769230769232\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for n in range(epochs):\n",
    "    obs = env.reset()\n",
    "    _obs = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
    "    trajectories = []\n",
    "    sa_count = 0\n",
    "    states  = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        sa_count += 1\n",
    "        action = Categorical(policy(_obs)).sample().item()\n",
    "        states.append(obs.copy())\n",
    "        actions.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        _obs = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            trajectories.append((states, rewards, actions))\n",
    "            states  = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            obs = env.reset()\n",
    "            _obs = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
    "            if sa_count > batch_size:\n",
    "                sa_count = 0\n",
    "                break\n",
    "\n",
    "    for trj in trajectories:\n",
    "        grad = getGrad(policy, trj[0], trj[1], trj[2])\n",
    "        grad.backward()\n",
    "        optimizer.step()\n",
    "    print(\"avg rewards:\", sum(sum(trj[1]) for trj in trajectories)/len(trajectories))\n",
    "    trajectories = []\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test rewards 200.0\n",
      "test rewards 200.0\n",
      "test rewards 200.0\n",
      "test rewards 200.0\n",
      "test rewards 200.0\n",
      "test rewards 200.0\n",
      "test rewards 200.0\n",
      "test rewards 200.0\n",
      "test rewards 200.0\n",
      "test rewards 200.0\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "env.render()\n",
    "obs = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
    "print(\"test\")\n",
    "for _ in range(10):\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        # print(obs)\n",
    "        action = torch.argmax(policy(obs)).item()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            print(\"test rewards\", sum(rewards))\n",
    "            rewards = []\n",
    "            obs = env.reset()\n",
    "            obs = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
